## 作用

为了增加神经网络模型的非线性，

没有激活函数的每层相当于矩阵相乘，就算你增加了若干层之后，还是个矩阵相乘罢了，

没有非线性结构不能称之为一个神经网络



## 为什么引入激活函数

>  如果不用激活函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了 



### sigmoid

- 指数运算，计算量大，反向传播求误差梯度时，求导涉及除法，计算量大
-  对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失，从而无法完成深层网络的训练 
- 容易梯度消失  gradient vanishing，会导致导数逐渐变成0，使得参数无法被更新
- 函数输出并不是 zero-centered    Sigmoid函数的输出值恒大于0，这会导致模型训练的收敛速度变慢 



### Relu

- 计算量节省， 只需要判断输入是否大于0 
-  Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生
-  解决了gradient vanishing问题 (在正区间)  
-  收敛速度远快于sigmoid和tanh 
-  ReLU的输出不是zero-centered 



####  Gradient Vanishing  反向传播

 先计算输出层对应的loss，然后将loss以导数的形式不断向上一层网络传递，修正相应的参数，达到降低loss的目的 。

##### 对于sigmoid中的反向传播

-  当中较大或较小时，导数接近0，而后向传递的数学依据是微积分求导的链式法则，当前层的导数需要之前各层导数的乘积，几个小数的相乘，结果会很接近0 
-  Sigmoid导数的最大值是0.25，这意味着导数在每一层至少会被压缩为原来的1/4，通过两层后被变为1/16，…，通过10层后为1/1048576。请注意这里是“至少”，导数达到最大值这种情况还是很少见的 









[为什么要用Relu](https://www.sohu.com/a/214965417_100008678)

